{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PPUqQtVHKggi"
   },
   "source": [
    "# VGGish Audio Embedding Collab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuhXsmvIKL62"
   },
   "source": [
    "This colab demonstrates how to extract the AudioSet embeddings, using a VGGish deep neural network (DNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAE4O-fK-RW2"
   },
   "source": [
    "# Importing and Testing the VGGish System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKSLc0bIB1QS"
   },
   "source": [
    "Based on the directions at: https://github.com/tensorflow/models/tree/master/research/audioset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1YVQb-MBiUx",
    "outputId": "cb57ab27-62e0-4cd4-b9a7-d69555600a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (1.14.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (1.1.0)\n",
      "Requirement already satisfied: resampy in /usr/local/lib/python2.7/dist-packages (0.2.1)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python2.7/dist-packages (1.10.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (1.11.0)\n",
      "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python2.7/dist-packages (from resampy) (0.40.0)\n",
      "Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python2.7/dist-packages (from resampy) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python2.7/dist-packages (from resampy) (1.14.5)\n",
      "Requirement already satisfied: setuptools<=39.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (39.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.6)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<1.11.0,>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.10.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.0.post1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python2.7/dist-packages (from numba>=0.32->resampy) (0.25.0)\n",
      "Requirement already satisfied: funcsigs in /usr/local/lib/python2.7/dist-packages (from numba>=0.32->resampy) (1.0.2)\n",
      "Requirement already satisfied: singledispatch in /usr/local/lib/python2.7/dist-packages (from numba>=0.32->resampy) (3.4.0.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: futures>=3.1.1; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow) (2.6.11)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow) (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy\n",
    "!pip install resampy tensorflow six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  git-man less libbsd0 libedit2 liberror-perl libx11-6 libx11-data libxau6\n",
      "  libxcb1 libxdmcp6 libxext6 libxmuu1 openssh-client xauth\n",
      "Suggested packages:\n",
      "  gettext-base git-daemon-run | git-daemon-sysvinit git-doc git-el git-email\n",
      "  git-gui gitk gitweb git-arch git-cvs git-mediawiki git-svn ssh-askpass\n",
      "  libpam-ssh keychain monkeysphere\n",
      "The following NEW packages will be installed:\n",
      "  git git-man less libbsd0 libedit2 liberror-perl libx11-6 libx11-data libxau6\n",
      "  libxcb1 libxdmcp6 libxext6 libxmuu1 openssh-client xauth\n",
      "0 upgraded, 15 newly installed, 0 to remove and 16 not upgraded.\n",
      "Need to get 5536 kB of archives.\n",
      "After this operation, 33.6 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu xenial/main amd64 libxau6 amd64 1:1.0.8-1 [8376 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu xenial/main amd64 libxdmcp6 amd64 1:1.1.2-1.1 [11.0 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu xenial/main amd64 libxcb1 amd64 1.11.1-1ubuntu1 [40.0 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libx11-data all 2:1.6.3-1ubuntu2.1 [113 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libx11-6 amd64 2:1.6.3-1ubuntu2.1 [570 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu xenial/main amd64 libxext6 amd64 2:1.3.3-1 [29.4 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 less amd64 481-2.1ubuntu0.2 [110 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu xenial/main amd64 libbsd0 amd64 0.8.2-1 [41.7 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu xenial/main amd64 libedit2 amd64 3.1-20150325-1ubuntu2 [76.5 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu xenial/main amd64 libxmuu1 amd64 2:1.1.2-2 [9674 B]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 openssh-client amd64 1:7.2p2-4ubuntu2.4 [589 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu xenial/main amd64 xauth amd64 1:1.0.9-1ubuntu2 [22.7 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu xenial/main amd64 liberror-perl all 0.17-1.2 [19.6 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 git-man all 1:2.7.4-0ubuntu1.4 [736 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 git amd64 1:2.7.4-0ubuntu1.4 [3158 kB]\n",
      "Fetched 5536 kB in 1s (3449 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libxau6:amd64.\n",
      "(Reading database ... 14809 files and directories currently installed.)\n",
      "Preparing to unpack .../libxau6_1%3a1.0.8-1_amd64.deb ...\n",
      "Unpacking libxau6:amd64 (1:1.0.8-1) ...\n",
      "Selecting previously unselected package libxdmcp6:amd64.\n",
      "Preparing to unpack .../libxdmcp6_1%3a1.1.2-1.1_amd64.deb ...\n",
      "Unpacking libxdmcp6:amd64 (1:1.1.2-1.1) ...\n",
      "Selecting previously unselected package libxcb1:amd64.\n",
      "Preparing to unpack .../libxcb1_1.11.1-1ubuntu1_amd64.deb ...\n",
      "Unpacking libxcb1:amd64 (1.11.1-1ubuntu1) ...\n",
      "Selecting previously unselected package libx11-data.\n",
      "Preparing to unpack .../libx11-data_2%3a1.6.3-1ubuntu2.1_all.deb ...\n",
      "Unpacking libx11-data (2:1.6.3-1ubuntu2.1) ...\n",
      "Selecting previously unselected package libx11-6:amd64.\n",
      "Preparing to unpack .../libx11-6_2%3a1.6.3-1ubuntu2.1_amd64.deb ...\n",
      "Unpacking libx11-6:amd64 (2:1.6.3-1ubuntu2.1) ...\n",
      "Selecting previously unselected package libxext6:amd64.\n",
      "Preparing to unpack .../libxext6_2%3a1.3.3-1_amd64.deb ...\n",
      "Unpacking libxext6:amd64 (2:1.3.3-1) ...\n",
      "Selecting previously unselected package less.\n",
      "Preparing to unpack .../less_481-2.1ubuntu0.2_amd64.deb ...\n",
      "Unpacking less (481-2.1ubuntu0.2) ...\n",
      "Selecting previously unselected package libbsd0:amd64.\n",
      "Preparing to unpack .../libbsd0_0.8.2-1_amd64.deb ...\n",
      "Unpacking libbsd0:amd64 (0.8.2-1) ...\n",
      "Selecting previously unselected package libedit2:amd64.\n",
      "Preparing to unpack .../libedit2_3.1-20150325-1ubuntu2_amd64.deb ...\n",
      "Unpacking libedit2:amd64 (3.1-20150325-1ubuntu2) ...\n",
      "Selecting previously unselected package libxmuu1:amd64.\n",
      "Preparing to unpack .../libxmuu1_2%3a1.1.2-2_amd64.deb ...\n",
      "Unpacking libxmuu1:amd64 (2:1.1.2-2) ...\n",
      "Selecting previously unselected package openssh-client.\n",
      "Preparing to unpack .../openssh-client_1%3a7.2p2-4ubuntu2.4_amd64.deb ...\n",
      "Unpacking openssh-client (1:7.2p2-4ubuntu2.4) ...\n",
      "Selecting previously unselected package xauth.\n",
      "Preparing to unpack .../xauth_1%3a1.0.9-1ubuntu2_amd64.deb ...\n",
      "Unpacking xauth (1:1.0.9-1ubuntu2) ...\n",
      "Selecting previously unselected package liberror-perl.\n",
      "Preparing to unpack .../liberror-perl_0.17-1.2_all.deb ...\n",
      "Unpacking liberror-perl (0.17-1.2) ...\n",
      "Selecting previously unselected package git-man.\n",
      "Preparing to unpack .../git-man_1%3a2.7.4-0ubuntu1.4_all.deb ...\n",
      "Unpacking git-man (1:2.7.4-0ubuntu1.4) ...\n",
      "Selecting previously unselected package git.\n",
      "Preparing to unpack .../git_1%3a2.7.4-0ubuntu1.4_amd64.deb ...\n",
      "Unpacking git (1:2.7.4-0ubuntu1.4) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu10) ...\n",
      "Processing triggers for mime-support (3.59ubuntu1) ...\n",
      "Setting up libxau6:amd64 (1:1.0.8-1) ...\n",
      "Setting up libxdmcp6:amd64 (1:1.1.2-1.1) ...\n",
      "Setting up libxcb1:amd64 (1.11.1-1ubuntu1) ...\n",
      "Setting up libx11-data (2:1.6.3-1ubuntu2.1) ...\n",
      "Setting up libx11-6:amd64 (2:1.6.3-1ubuntu2.1) ...\n",
      "Setting up libxext6:amd64 (2:1.3.3-1) ...\n",
      "Setting up less (481-2.1ubuntu0.2) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Setting up libbsd0:amd64 (0.8.2-1) ...\n",
      "Setting up libedit2:amd64 (3.1-20150325-1ubuntu2) ...\n",
      "Setting up libxmuu1:amd64 (2:1.1.2-2) ...\n",
      "Setting up openssh-client (1:7.2p2-4ubuntu2.4) ...\n",
      "Setting up xauth (1:1.0.9-1ubuntu2) ...\n",
      "Setting up liberror-perl (0.17-1.2) ...\n",
      "Setting up git-man (1:2.7.4-0ubuntu1.4) ...\n",
      "Setting up git (1:2.7.4-0ubuntu1.4) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu10) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kN7yMJ3BBu_Q",
    "outputId": "4cefe4aa-c6d3-4d79-f347-e77c6198350c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'youtube-8m' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/google/youtube-8m.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOutr-RFCFfD",
    "outputId": "bcf49405-c05f-4656-947b-c948b454d545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "# Check to see where are in the kernel's file system.\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7CVgYDQ2CG4K",
    "outputId": "2b2c923c-d310-4529-a3e5-eaeb8d52406b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  277M  100  277M    0     0   100M      0  0:00:02  0:00:02 --:--:--  100M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 73020  100 73020    0     0   478k      0 --:--:-- --:--:-- --:--:--  475k\n"
     ]
    }
   ],
   "source": [
    "# Grab the VGGish model\n",
    "!curl -O https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
    "!curl -O https://storage.googleapis.com/audioset/vggish_pca_params.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xEl3w-RjCPwp",
    "outputId": "890e71da-c5f5-4642-dcea-cab5a51ab943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGish_Audioset_&_Audio_embedding_Tutorial.ipynb  vggish_model.ckpt\r\n",
      "audioset_v1_embeddings\t\t\t\t  vggish_pca_params.npz\r\n",
      "features.tar.gz\t\t\t\t\t  vggish_smoke_test.pyc\r\n",
      "models\t\t\t\t\t\t  youtube-8m\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure we got the model data.\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oApFn6gzCvsa",
    "outputId": "dd20d037-942c-432b-8b13-9e67e625f012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2468M  100 2468M    0     0   159M      0  0:00:15  0:00:15 --:--:--  184M\n"
     ]
    }
   ],
   "source": [
    "# Copy the source files to the current directory.\n",
    "!curl -O http://storage.googleapis.com/us_audioset/youtube_corpus/v1/features/features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'models' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DaMrmOEvC7L4",
    "outputId": "c1b9dca8-a930-4f01-9390-110c314a0b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGish_Audioset_&_Audio_embedding_Tutorial.ipynb  vggish_model.ckpt\r\n",
      "audioset_v1_embeddings\t\t\t\t  vggish_pca_params.npz\r\n",
      "features.tar.gz\t\t\t\t\t  vggish_smoke_test.pyc\r\n",
      "models\t\t\t\t\t\t  youtube-8m\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure the source files got copied correctly.\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbtPmmX-CTHB",
    "outputId": "6e548dd6-d500-4bf6-cded-b367da66d324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t\t  vggish_input.py\t vggish_slim.py\r\n",
      "mel_features.py\t\t  vggish_params.py\t vggish_smoke_test.py\r\n",
      "vggish_inference_demo.py  vggish_postprocess.py  vggish_train_demo.py\r\n"
     ]
    }
   ],
   "source": [
    "# Verify the location of the AudioSet source files\n",
    "%cd models/research/audioset\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZb1ohlU2NXr"
   },
   "source": [
    "# Audioset Embedding Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "cC4tZrfAoyt8",
    "outputId": "bb8c08b0-a951-4dac-f6bf-f81ada1d7268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2468M  100 2468M    0     0   173M      0  0:00:14  0:00:14 --:--:--  155M\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://storage.googleapis.com/us_audioset/youtube_corpus/v1/features/features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6xKmtqdpJ_w"
   },
   "outputs": [],
   "source": [
    "#Unpack the Audioset Features\n",
    "!tar -xzf features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JM0ZjuAJppwv",
    "outputId": "e8636638-0ff2-4f2e-9a4e-0d4268c83cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                 vggish_input.py         vggish_slim.py\r\n",
      "\u001b[0m\u001b[01;34maudioset_v1_embeddings\u001b[0m/   vggish_input.pyc        vggish_slim.pyc\r\n",
      "features.tar.gz           vggish_params.py        vggish_smoke_test.py\r\n",
      "mel_features.py           vggish_params.pyc       vggish_smoke_test.pyc\r\n",
      "mel_features.pyc          vggish_postprocess.py   vggish_train_demo.py\r\n",
      "vggish_inference_demo.py  vggish_postprocess.pyc\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGish_Audioset_&_Audio_embedding_Tutorial.ipynb  vggish_model.ckpt\r\n",
      "\u001b[0m\u001b[01;34maudioset_v1_embeddings\u001b[0m/                           vggish_pca_params.npz\r\n",
      "features.tar.gz                                   vggish_smoke_test.pyc\r\n",
      "\u001b[01;34mmodels\u001b[0m/                                           \u001b[01;34myoutube-8m\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rF39FQ0_qdA_",
    "outputId": "a3de180a-d837-4446-db43-1e1fae1ede93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/youtube-8m\n"
     ]
    }
   ],
   "source": [
    "cd youtube-8m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVsOSokZsgjI"
   },
   "outputs": [],
   "source": [
    "!rm readers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4Rg_spRKvE0Z",
    "outputId": "7830390d-058f-453d-892a-5af9ce9c4186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing readers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile readers.py\n",
    "# Copyright 2016 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Provides readers configured for different datasets.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from tensorflow import logging\n",
    "def resize_axis(tensor, axis, new_size, fill_value=0):\n",
    "  \"\"\"Truncates or pads a tensor to new_size on on a given axis.\n",
    "  Truncate or extend tensor such that tensor.shape[axis] == new_size. If the\n",
    "  size increases, the padding will be performed at the end, using fill_value.\n",
    "  Args:\n",
    "    tensor: The tensor to be resized.\n",
    "    axis: An integer representing the dimension to be sliced.\n",
    "    new_size: An integer or 0d tensor representing the new value for\n",
    "      tensor.shape[axis].\n",
    "    fill_value: Value to use to fill any new entries in the tensor. Will be\n",
    "      cast to the type of tensor.\n",
    "  Returns:\n",
    "    The resized tensor.\n",
    "  \"\"\"\n",
    "  tensor = tf.convert_to_tensor(tensor)\n",
    "  shape = tf.unstack(tf.shape(tensor))\n",
    "\n",
    "  pad_shape = shape[:]\n",
    "  pad_shape[axis] = tf.maximum(0, new_size - shape[axis])\n",
    "\n",
    "  shape[axis] = tf.minimum(shape[axis], new_size)\n",
    "  shape = tf.stack(shape)\n",
    "\n",
    "  resized = tf.concat([\n",
    "      tf.slice(tensor, tf.zeros_like(shape), shape),\n",
    "      tf.fill(tf.stack(pad_shape), tf.cast(fill_value, tensor.dtype))\n",
    "  ], axis)\n",
    "\n",
    "  # Update shape.\n",
    "  new_shape = tensor.get_shape().as_list()  # A copy is being made.\n",
    "  new_shape[axis] = new_size\n",
    "  resized.set_shape(new_shape)\n",
    "  return resized\n",
    "\n",
    "class BaseReader(object):\n",
    "  \"\"\"Inherit from this class when implementing new readers.\"\"\"\n",
    "\n",
    "  def prepare_reader(self, unused_filename_queue):\n",
    "    \"\"\"Create a thread for generating prediction and label tensors.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "class YT8MAggregatedFeatureReader(BaseReader):\n",
    "  \"\"\"Reads TFRecords of pre-aggregated Examples.\n",
    "  The TFRecords must contain Examples with a sparse int64 'labels' feature and\n",
    "  a fixed length float32 feature, obtained from the features in 'feature_name'.\n",
    "  The float features are assumed to be an average of dequantized values.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_classes=527,\n",
    "               feature_sizes=[1024, 128],\n",
    "               feature_names=[\"mean_rgb\", \"mean_audio\"]):\n",
    "    \"\"\"Construct a YT8MAggregatedFeatureReader.\n",
    "    Args:\n",
    "      num_classes: a positive integer for the number of classes.\n",
    "      feature_sizes: positive integer(s) for the feature dimensions as a list.\n",
    "      feature_names: the feature name(s) in the tensorflow record as a list.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(feature_names) == len(feature_sizes), \\\n",
    "    \"length of feature_names (={}) != length of feature_sizes (={})\".format( \\\n",
    "    len(feature_names), len(feature_sizes))\n",
    "\n",
    "    self.num_classes = num_classes\n",
    "    self.feature_sizes = feature_sizes\n",
    "    self.feature_names = feature_names\n",
    "\n",
    "  def prepare_reader(self, filename_queue, batch_size=1024):\n",
    "    \"\"\"Creates a single reader thread for pre-aggregated YouTube 8M Examples.\n",
    "    Args:\n",
    "      filename_queue: A tensorflow queue of filename locations.\n",
    "    Returns:\n",
    "      A tuple of video indexes, features, labels, and padding data.\n",
    "    \"\"\"\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_examples = reader.read_up_to(filename_queue, batch_size)\n",
    "\n",
    "    tf.add_to_collection(\"serialized_examples\", serialized_examples)\n",
    "    return self.prepare_serialized_examples(serialized_examples)\n",
    "\n",
    "  def prepare_serialized_examples(self, serialized_examples):\n",
    "    # set the mapping from the fields to data types in the proto\n",
    "    num_features = len(self.feature_names)\n",
    "    assert num_features > 0, \"self.feature_names is empty!\"\n",
    "    assert len(self.feature_names) == len(self.feature_sizes), \\\n",
    "    \"length of feature_names (={}) != length of feature_sizes (={})\".format( \\\n",
    "    len(self.feature_names), len(self.feature_sizes))\n",
    "\n",
    "    feature_map = {\"video_id\": tf.FixedLenFeature([], tf.string),\n",
    "                   \"labels\": tf.VarLenFeature(tf.int64)}\n",
    "    for feature_index in range(num_features):\n",
    "      feature_map[self.feature_names[feature_index]] = tf.FixedLenFeature(\n",
    "          [self.feature_sizes[feature_index]], tf.float32)\n",
    "\n",
    "    features = tf.parse_example(serialized_examples, features=feature_map)\n",
    "    labels = tf.sparse_to_indicator(features[\"labels\"], self.num_classes)\n",
    "    labels.set_shape([None, self.num_classes])\n",
    "    concatenated_features = tf.concat([\n",
    "        features[feature_name] for feature_name in self.feature_names], 1)\n",
    "\n",
    "    return features[\"video_id\"], concatenated_features, labels, tf.ones([tf.shape(serialized_examples)[0]])\n",
    "\n",
    "class YT8MFrameFeatureReader(BaseReader):\n",
    "  \"\"\"Reads TFRecords of SequenceExamples.\n",
    "  The TFRecords must contain SequenceExamples with the sparse in64 'labels'\n",
    "  context feature and a fixed length byte-quantized feature vector, obtained\n",
    "  from the features in 'feature_names'. The quantized features will be mapped\n",
    "  back into a range between min_quantized_value and max_quantized_value.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_classes=527,\n",
    "               feature_sizes=[1024, 128],\n",
    "               feature_names=[\"rgb\", \"audio\"],\n",
    "               max_frames=300):\n",
    "    \"\"\"Construct a YT8MFrameFeatureReader.\n",
    "    Args:\n",
    "      num_classes: a positive integer for the number of classes.\n",
    "      feature_sizes: positive integer(s) for the feature dimensions as a list.\n",
    "      feature_names: the feature name(s) in the tensorflow record as a list.\n",
    "      max_frames: the maximum number of frames to process.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(feature_names) == len(feature_sizes), \\\n",
    "    \"length of feature_names (={}) != length of feature_sizes (={})\".format( \\\n",
    "    len(feature_names), len(feature_sizes))\n",
    "\n",
    "    self.num_classes = num_classes\n",
    "    self.feature_sizes = feature_sizes\n",
    "    self.feature_names = feature_names\n",
    "    self.max_frames = max_frames\n",
    "\n",
    "  def get_video_matrix(self,\n",
    "                       features,\n",
    "                       feature_size,\n",
    "                       max_frames,\n",
    "                       max_quantized_value,\n",
    "                       min_quantized_value):\n",
    "    \"\"\"Decodes features from an input string and quantizes it.\n",
    "    Args:\n",
    "      features: raw feature values\n",
    "      feature_size: length of each frame feature vector\n",
    "      max_frames: number of frames (rows) in the output feature_matrix\n",
    "      max_quantized_value: the maximum of the quantized value.\n",
    "      min_quantized_value: the minimum of the quantized value.\n",
    "    Returns:\n",
    "      feature_matrix: matrix of all frame-features\n",
    "      num_frames: number of frames in the sequence\n",
    "    \"\"\"\n",
    "    decoded_features = tf.reshape(\n",
    "        tf.cast(tf.decode_raw(features, tf.uint8), tf.float32),\n",
    "        [-1, feature_size])\n",
    "\n",
    "    num_frames = tf.minimum(tf.shape(decoded_features)[0], max_frames)\n",
    "    feature_matrix = utils.Dequantize(decoded_features,\n",
    "                                      max_quantized_value,\n",
    "                                      min_quantized_value)\n",
    "    feature_matrix = resize_axis(feature_matrix, 0, max_frames)\n",
    "    return feature_matrix, num_frames\n",
    "\n",
    "  def prepare_reader(self,\n",
    "                     filename_queue,\n",
    "                     max_quantized_value=2,\n",
    "                     min_quantized_value=-2):\n",
    "    \"\"\"Creates a single reader thread for YouTube8M SequenceExamples.\n",
    "    Args:\n",
    "      filename_queue: A tensorflow queue of filename locations.\n",
    "      max_quantized_value: the maximum of the quantized value.\n",
    "      min_quantized_value: the minimum of the quantized value.\n",
    "    Returns:\n",
    "      A tuple of video indexes, video features, labels, and padding data.\n",
    "    \"\"\"\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    return self.prepare_serialized_examples(serialized_example,\n",
    "        max_quantized_value, min_quantized_value)\n",
    "\n",
    "  def prepare_serialized_examples(self, serialized_example,\n",
    "      max_quantized_value=2, min_quantized_value=-2):\n",
    "\n",
    "    contexts, features = tf.parse_single_sequence_example(\n",
    "        serialized_example,\n",
    "        context_features={\"video_id\": tf.FixedLenFeature(\n",
    "            [], tf.string),\n",
    "                          \"labels\": tf.VarLenFeature(tf.int64)},\n",
    "        sequence_features={\n",
    "            feature_name : tf.FixedLenSequenceFeature([], dtype=tf.string)\n",
    "            for feature_name in self.feature_names\n",
    "        })\n",
    "\n",
    "    # read ground truth labels\n",
    "    labels = (tf.cast(\n",
    "        tf.sparse_to_dense(contexts[\"labels\"].values, (self.num_classes,), 1,\n",
    "            validate_indices=False),\n",
    "        tf.bool))\n",
    "\n",
    "    # loads (potentially) different types of features and concatenates them\n",
    "    num_features = len(self.feature_names)\n",
    "    assert num_features > 0, \"No feature selected: feature_names is empty!\"\n",
    "\n",
    "    assert len(self.feature_names) == len(self.feature_sizes), \\\n",
    "    \"length of feature_names (={}) != length of feature_sizes (={})\".format( \\\n",
    "    len(self.feature_names), len(self.feature_sizes))\n",
    "\n",
    "    num_frames = -1  # the number of frames in the video\n",
    "    feature_matrices = [None] * num_features  # an array of different features\n",
    "    for feature_index in range(num_features):\n",
    "      feature_matrix, num_frames_in_this_feature = self.get_video_matrix(\n",
    "          features[self.feature_names[feature_index]],\n",
    "          self.feature_sizes[feature_index],\n",
    "          self.max_frames,\n",
    "          max_quantized_value,\n",
    "          min_quantized_value)\n",
    "      if num_frames == -1:\n",
    "        num_frames = num_frames_in_this_feature\n",
    "      else:\n",
    "        tf.assert_equal(num_frames, num_frames_in_this_feature)\n",
    "\n",
    "      feature_matrices[feature_index] = feature_matrix\n",
    "\n",
    "    # cap the number of frames at self.max_frames\n",
    "    num_frames = tf.minimum(num_frames, self.max_frames)\n",
    "\n",
    "    # concatenate different features\n",
    "    video_matrix = tf.concat(feature_matrices, 1)\n",
    "\n",
    "    # convert to batch format.\n",
    "    # TODO: Do proper batch reads to remove the IO bottleneck.\n",
    "    batch_video_ids = tf.expand_dims(contexts[\"video_id\"], 0)\n",
    "    batch_video_matrix = tf.expand_dims(video_matrix, 0)\n",
    "    batch_labels = tf.expand_dims(labels, 0)\n",
    "    batch_frames = tf.expand_dims(num_frames, 0)\n",
    "\n",
    "    return batch_video_ids, batch_video_matrix, batch_labels, batch_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TAmjBdFtv6m1",
    "outputId": "714fda79-a05c-4623-aba9-75636d565a97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "tITmc0TYwI9D",
    "outputId": "dffcddf1-9112-405e-8be2-c06885ada7d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'models' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tensorflow/models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Uzhhs7P5w6av",
    "outputId": "4d363dc3-14b9-42c8-cdaa-5774eefaf8a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/models/research/audioset\n"
     ]
    }
   ],
   "source": [
    "cd models/research/audioset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmUClFszxQsQ"
   },
   "outputs": [],
   "source": [
    "rm vggish_inference_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jJnvTT6mxZEb",
    "outputId": "b129a3f1-aa13-41aa-b924-443bd1b67be6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vggish_inference_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vggish_inference_demo.py\n",
    "\n",
    "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "r\"\"\"A simple demonstration of running VGGish in inference mode.\n",
    "\n",
    "This is intended as a toy example that demonstrates how the various building\n",
    "blocks (feature extraction, model definition and loading, postprocessing) work\n",
    "together in an inference context.\n",
    "\n",
    "A WAV file (assumed to contain signed 16-bit PCM samples) is read in, converted\n",
    "into log mel spectrogram examples, fed into VGGish, the raw embedding output is\n",
    "whitened and quantized, and the postprocessed embeddings are optionally written\n",
    "in a SequenceExample to a TFRecord file (using the same format as the embedding\n",
    "features released in AudioSet).\n",
    "\n",
    "Usage:\n",
    "  # Run a WAV file through the model and print the embeddings. The model\n",
    "  # checkpoint is loaded from vggish_model.ckpt and the PCA parameters are\n",
    "  # loaded from vggish_pca_params.npz in the current directory.\n",
    "  $ python vggish_inference_demo.py --wav_file /path/to/a/wav/file\n",
    "\n",
    "  # Run a WAV file through the model and also write the embeddings to\n",
    "  # a TFRecord file. The model checkpoint and PCA parameters are explicitly\n",
    "  # passed in as well.\n",
    "  $ python vggish_inference_demo.py --wav_file /path/to/a/wav/file \\\n",
    "                                    --tfrecord_file /path/to/tfrecord/file \\\n",
    "                                    --checkpoint /path/to/model/checkpoint \\\n",
    "                                    --pca_params /path/to/pca/params\n",
    "\n",
    "  # Run a built-in input (a sine wav) through the model and print the\n",
    "  # embeddings. Associated model files are read from the current directory.\n",
    "  $ python vggish_inference_demo.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "import vggish_slim\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'wav_file', None,\n",
    "    'Path to a wav file. Should contain signed 16-bit PCM samples. '\n",
    "    'If none is provided, a synthetic sound is used.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'checkpoint', 'vggish_model.ckpt',\n",
    "    'Path to the VGGish checkpoint file.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'pca_params', 'vggish_pca_params.npz',\n",
    "    'Path to the VGGish PCA parameters file.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'tfrecord_file', None,\n",
    "    'Path to a TFRecord file where embeddings will be written.')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # In this simple example, we run the examples from a single audio file through\n",
    "    # the model. If none is provided, we generate a synthetic input.\n",
    "    if FLAGS.wav_file:\n",
    "        wav_file = FLAGS.wav_file\n",
    "    else:\n",
    "        # Write a WAV of a sine wav into an in-memory file object.\n",
    "        num_secs = 5\n",
    "        freq = 1000\n",
    "        sr = 44100\n",
    "        t = np.linspace(0, num_secs, int(num_secs * sr))\n",
    "        x = np.sin(2 * np.pi * freq * t)\n",
    "        # Convert to signed 16-bit samples.\n",
    "        samples = np.clip(x * 32768, -32768, 32767).astype(np.int16)\n",
    "        wav_file = six.BytesIO()\n",
    "        wavfile.write(wav_file, sr, samples)\n",
    "        wav_file.seek(0)\n",
    "    examples_batch = vggish_input.wavfile_to_examples(wav_file)\n",
    "    print(examples_batch)\n",
    "\n",
    "    # Prepare a postprocessor to munge the model embeddings.\n",
    "    pproc = vggish_postprocess.Postprocessor(FLAGS.pca_params)\n",
    "\n",
    "    # If needed, prepare a record writer to store the postprocessed embeddings.\n",
    "    writer = tf.python_io.TFRecordWriter(\n",
    "        FLAGS.tfrecord_file) if FLAGS.tfrecord_file else None\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        # Define the model in inference mode, load the checkpoint, and\n",
    "        # locate input and output tensors.\n",
    "        vggish_slim.define_vggish_slim(training=False)\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "        # Run inference and postprocessing.\n",
    "        [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                     feed_dict={features_tensor: examples_batch})\n",
    "        print(embedding_batch)\n",
    "        postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "        print(postprocessed_batch)\n",
    "\n",
    "        # Write the postprocessed embeddings as a SequenceExample, in a similar\n",
    "        # format as the features released in AudioSet. Each row of the batch of\n",
    "        # embeddings corresponds to roughly a second of audio (96 10ms frames), and\n",
    "        # the rows are written as a sequence of bytes-valued features, where each\n",
    "        # feature value contains the 128 bytes of the whitened quantized embedding.\n",
    "        seq_example = tf.train.SequenceExample(\n",
    "            context=tf.train.Features(feature={\n",
    "                'video_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[wav_file.encode()]))\n",
    "            }),\n",
    "            feature_lists=tf.train.FeatureLists(\n",
    "                feature_list={\n",
    "                    vggish_params.AUDIO_EMBEDDING_FEATURE_NAME:\n",
    "                        tf.train.FeatureList(\n",
    "                            feature=[\n",
    "                                tf.train.Feature(\n",
    "                                    bytes_list=tf.train.BytesList(\n",
    "                                        value=[embedding.tobytes()]))\n",
    "                                for embedding in postprocessed_batch\n",
    "                            ]\n",
    "                        )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        print(seq_example)\n",
    "        if writer:\n",
    "            writer.write(seq_example.SerializeToString())\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "dxzl-0pnxuEG",
    "outputId": "c2f9162e-fcd6-4485-937e-7d81bac77d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                 vggish_input.py         vggish_slim.py\r\n",
      "\u001b[0m\u001b[01;34maudioset_v1_embeddings\u001b[0m/   vggish_input.pyc        vggish_slim.pyc\r\n",
      "features.tar.gz           vggish_params.py        vggish_smoke_test.py\r\n",
      "mel_features.py           vggish_params.pyc       vggish_smoke_test.pyc\r\n",
      "mel_features.pyc          vggish_postprocess.py   vggish_train_demo.py\r\n",
      "vggish_inference_demo.py  vggish_postprocess.pyc\r\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M-FQFRGwxvno",
    "outputId": "4e39a282-d1fa-4926-c20a-f1fc3b6fb355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGish_Audioset_&_Audio_embedding_Tutorial.ipynb  vggish_model.ckpt\r\n",
      "\u001b[0m\u001b[01;34maudioset_v1_embeddings\u001b[0m/                           vggish_pca_params.npz\r\n",
      "features.tar.gz                                   vggish_smoke_test.pyc\r\n",
      "\u001b[01;34mmodels\u001b[0m/                                           \u001b[01;34myoutube-8m\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1697
    },
    "colab_type": "code",
    "id": "Ukp5n4xqx4PX",
    "outputId": "a13079e3-d1d3-4176-e04e-96845620f277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/job:master/task:0: Tensorflow version: 1.10.1.\n",
      "INFO:tensorflow:/job:master/task:0: Removing existing train directory.\n",
      "INFO:tensorflow:/job:master/task:0: Flag 'start_new_model' is set. Building a new model.\n",
      "2018-09-25 23:26:21.673710: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2018-09-25 23:26:25.473318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 6de9:00:00.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2018-09-25 23:26:25.473366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n",
      "2018-09-25 23:26:25.852693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-09-25 23:26:25.852748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \n",
      "2018-09-25 23:26:25.852777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \n",
      "2018-09-25 23:26:25.853026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:0 with 10759 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 6de9:00:00.0, compute capability: 3.7)\n",
      "INFO:tensorflow:Using the following GPUs to train: [u'/device:GPU:0']\n",
      "INFO:tensorflow:Using batch size of 1024 for training.\n",
      "INFO:tensorflow:Number of training files: 4070.\n",
      "WARNING:tensorflow:From youtube-8m/train.py:436: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:/job:master/task:0: Starting managed session.\n",
      "2018-09-25 23:26:27.394363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n",
      "2018-09-25 23:26:27.394422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-09-25 23:26:27.394442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \n",
      "2018-09-25 23:26:27.394458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \n",
      "2018-09-25 23:26:27.394576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10759 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 6de9:00:00.0, compute capability: 3.7)\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path model_new/dir/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:/job:master/task:0: Entering training loop.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:model_new/dir/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:Recording summary at step 0.\n",
      "INFO:tensorflow:training step 1 | Loss: 215.31 Examples/sec: 347.96\n",
      "INFO:tensorflow:training step 2 | Loss: 214.07 Examples/sec: 1353.68\n",
      "INFO:tensorflow:training step 3 | Loss: 201.43 Examples/sec: 1324.04\n",
      "INFO:tensorflow:training step 4 | Loss: 130.13 Examples/sec: 1312.61\n",
      "INFO:tensorflow:training step 5 | Loss: 35.51 Examples/sec: 1318.29\n",
      "INFO:tensorflow:training step 6 | Loss: 24.28 Examples/sec: 1376.42\n",
      "INFO:tensorflow:training step 7 | Loss: 20.21 Examples/sec: 1368.67\n",
      "INFO:tensorflow:training step 8 | Loss: 18.19 Examples/sec: 1364.14\n",
      "INFO:tensorflow:training step 9 | Loss: 18.01 Examples/sec: 1286.54\n",
      "INFO:tensorflow:training step 10 | Loss: 17.10 Examples/sec: 1334.13 | Hit@1: 0.30 PERR: 0.23 GAP: 0.09\n",
      "INFO:tensorflow:model_new/dir/model.ckpt-10 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/job:master/task:0: Exporting the model at step 10 to model_new/dir/export/step_10.\n",
      "2018-09-25 23:26:39.324847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n",
      "2018-09-25 23:26:39.324898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-09-25 23:26:39.324909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \n",
      "2018-09-25 23:26:39.324918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \n",
      "2018-09-25 23:26:39.325047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10759 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 6de9:00:00.0, compute capability: 3.7)\n",
      "INFO:tensorflow:Restoring parameters from model_new/dir/model.ckpt-10\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: model_new/dir/export/step_10/saved_model.pb\n",
      "INFO:tensorflow:training step 11 | Loss: 15.98 Examples/sec: 1694.62\n",
      "INFO:tensorflow:training step 12 | Loss: 14.55 Examples/sec: 1684.83\n",
      "INFO:tensorflow:training step 13 | Loss: 14.43 Examples/sec: 1441.28\n",
      "INFO:tensorflow:training step 14 | Loss: 13.68 Examples/sec: 1353.63\n",
      "INFO:tensorflow:training step 15 | Loss: 13.18 Examples/sec: 1297.92\n",
      "INFO:tensorflow:training step 16 | Loss: 13.79 Examples/sec: 1319.40\n",
      "INFO:tensorflow:training step 17 | Loss: 13.93 Examples/sec: 1374.26\n",
      "INFO:tensorflow:training step 18 | Loss: 13.76 Examples/sec: 1285.93\n",
      "INFO:tensorflow:training step 19 | Loss: 13.76 Examples/sec: 1371.76\n",
      "INFO:tensorflow:training step 20 | Loss: 13.52 Examples/sec: 1340.72 | Hit@1: 0.29 PERR: 0.22 GAP: 0.07\n",
      "INFO:tensorflow:training step 21 | Loss: 13.68 Examples/sec: 1389.77\n",
      "INFO:tensorflow:training step 22 | Loss: 13.79 Examples/sec: 1363.54\n",
      "INFO:tensorflow:training step 23 | Loss: 13.42 Examples/sec: 1336.85\n",
      "INFO:tensorflow:training step 24 | Loss: 13.38 Examples/sec: 1373.27\n",
      "INFO:tensorflow:training step 25 | Loss: 13.32 Examples/sec: 1336.06\n",
      "INFO:tensorflow:training step 26 | Loss: 13.35 Examples/sec: 1320.58\n",
      "INFO:tensorflow:training step 27 | Loss: 13.53 Examples/sec: 1305.90\n",
      "INFO:tensorflow:training step 28 | Loss: 13.55 Examples/sec: 1348.10\n",
      "INFO:tensorflow:training step 29 | Loss: 13.65 Examples/sec: 1341.65\n",
      "INFO:tensorflow:training step 30 | Loss: 13.20 Examples/sec: 1346.36 | Hit@1: 0.27 PERR: 0.23 GAP: 0.08\n",
      "INFO:tensorflow:training step 31 | Loss: 13.07 Examples/sec: 1449.42\n",
      "INFO:tensorflow:training step 32 | Loss: 13.45 Examples/sec: 1340.94\n",
      "INFO:tensorflow:training step 33 | Loss: 13.78 Examples/sec: 1349.76\n",
      "INFO:tensorflow:training step 34 | Loss: 13.10 Examples/sec: 1355.14\n",
      "INFO:tensorflow:training step 35 | Loss: 13.29 Examples/sec: 1348.07\n",
      "INFO:tensorflow:training step 36 | Loss: 13.05 Examples/sec: 1326.39\n",
      "INFO:tensorflow:training step 37 | Loss: 13.03 Examples/sec: 1297.81\n",
      "INFO:tensorflow:training step 38 | Loss: 13.51 Examples/sec: 1330.48\n",
      "INFO:tensorflow:training step 39 | Loss: 13.21 Examples/sec: 1345.87\n",
      "INFO:tensorflow:training step 40 | Loss: 13.40 Examples/sec: 1319.82 | Hit@1: 0.23 PERR: 0.22 GAP: 0.08\n",
      "INFO:tensorflow:training step 41 | Loss: 13.26 Examples/sec: 1513.57\n",
      "INFO:tensorflow:training step 42 | Loss: 13.04 Examples/sec: 1316.13\n",
      "INFO:tensorflow:training step 43 | Loss: 13.24 Examples/sec: 1334.46\n",
      "INFO:tensorflow:training step 44 | Loss: 13.03 Examples/sec: 1370.54\n",
      "INFO:tensorflow:training step 45 | Loss: 13.06 Examples/sec: 1336.73\n",
      "INFO:tensorflow:training step 46 | Loss: 13.15 Examples/sec: 1335.15\n",
      "INFO:tensorflow:training step 47 | Loss: 12.92 Examples/sec: 1184.23\n",
      "INFO:tensorflow:training step 48 | Loss: 13.51 Examples/sec: 1279.32\n",
      "INFO:tensorflow:training step 49 | Loss: 13.05 Examples/sec: 1371.93\n",
      "INFO:tensorflow:training step 50 | Loss: 13.62 Examples/sec: 1357.33 | Hit@1: 0.24 PERR: 0.21 GAP: 0.08\n",
      "INFO:tensorflow:training step 51 | Loss: 13.36 Examples/sec: 1489.39\n",
      "INFO:tensorflow:training step 52 | Loss: 13.28 Examples/sec: 1301.20\n",
      "INFO:tensorflow:training step 53 | Loss: 13.00 Examples/sec: 1296.72\n",
      "INFO:tensorflow:training step 54 | Loss: 13.26 Examples/sec: 1320.87\n",
      "INFO:tensorflow:training step 55 | Loss: 13.31 Examples/sec: 1371.94\n",
      "INFO:tensorflow:training step 56 | Loss: 13.47 Examples/sec: 1334.07\n",
      "INFO:tensorflow:training step 57 | Loss: 13.21 Examples/sec: 1373.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:training step 58 | Loss: 13.10 Examples/sec: 1330.61\n",
      "INFO:tensorflow:training step 59 | Loss: 13.27 Examples/sec: 1290.93\n",
      "INFO:tensorflow:training step 60 | Loss: 13.25 Examples/sec: 1317.22 | Hit@1: 0.29 PERR: 0.22 GAP: 0.09\n",
      "INFO:tensorflow:training step 61 | Loss: 13.23 Examples/sec: 1490.06\n",
      "INFO:tensorflow:training step 62 | Loss: 13.36 Examples/sec: 1365.39\n",
      "INFO:tensorflow:training step 63 | Loss: 13.07 Examples/sec: 1326.67\n",
      "INFO:tensorflow:training step 64 | Loss: 13.38 Examples/sec: 1341.30\n",
      "INFO:tensorflow:training step 65 | Loss: 13.21 Examples/sec: 1312.92\n",
      "INFO:tensorflow:training step 66 | Loss: 13.10 Examples/sec: 1286.35\n",
      "INFO:tensorflow:training step 67 | Loss: 13.18 Examples/sec: 1336.88\n",
      "INFO:tensorflow:training step 68 | Loss: 13.32 Examples/sec: 1352.37\n",
      "INFO:tensorflow:training step 69 | Loss: 13.34 Examples/sec: 1338.00\n",
      "INFO:tensorflow:training step 70 | Loss: 13.32 Examples/sec: 1317.29 | Hit@1: 0.28 PERR: 0.23 GAP: 0.09\n",
      "INFO:tensorflow:training step 71 | Loss: 13.01 Examples/sec: 1467.32\n",
      "INFO:tensorflow:training step 72 | Loss: 13.58 Examples/sec: 1351.47\n",
      "INFO:tensorflow:training step 73 | Loss: 13.03 Examples/sec: 1351.02\n",
      "INFO:tensorflow:training step 74 | Loss: 13.52 Examples/sec: 1318.98\n",
      "INFO:tensorflow:training step 75 | Loss: 13.05 Examples/sec: 1325.81\n",
      "INFO:tensorflow:training step 76 | Loss: 13.09 Examples/sec: 1304.72\n",
      "INFO:tensorflow:training step 77 | Loss: 12.97 Examples/sec: 1345.52\n",
      "INFO:tensorflow:training step 78 | Loss: 12.98 Examples/sec: 1338.28\n",
      "INFO:tensorflow:training step 79 | Loss: 13.14 Examples/sec: 1334.62\n",
      "INFO:tensorflow:training step 80 | Loss: 13.63 Examples/sec: 1320.97 | Hit@1: 0.29 PERR: 0.22 GAP: 0.09\n",
      "INFO:tensorflow:training step 81 | Loss: 13.04 Examples/sec: 1449.16\n",
      "INFO:tensorflow:training step 82 | Loss: 13.17 Examples/sec: 1317.41\n",
      "INFO:tensorflow:training step 83 | Loss: 13.01 Examples/sec: 1334.72\n",
      "INFO:tensorflow:training step 84 | Loss: 13.35 Examples/sec: 1338.12\n",
      "INFO:tensorflow:training step 85 | Loss: 13.27 Examples/sec: 1272.58\n",
      "INFO:tensorflow:training step 86 | Loss: 12.88 Examples/sec: 1338.73\n",
      "INFO:tensorflow:training step 87 | Loss: 12.97 Examples/sec: 1310.86\n",
      "INFO:tensorflow:training step 88 | Loss: 13.17 Examples/sec: 1347.87\n",
      "INFO:tensorflow:training step 89 | Loss: 12.86 Examples/sec: 1336.35\n",
      "INFO:tensorflow:training step 90 | Loss: 12.92 Examples/sec: 1350.02 | Hit@1: 0.25 PERR: 0.22 GAP: 0.09\n",
      "INFO:tensorflow:training step 91 | Loss: 13.29 Examples/sec: 1468.97\n",
      "INFO:tensorflow:training step 92 | Loss: 13.29 Examples/sec: 1322.00\n",
      "INFO:tensorflow:training step 93 | Loss: 13.03 Examples/sec: 1324.57\n",
      "INFO:tensorflow:training step 94 | Loss: 13.08 Examples/sec: 1359.72\n",
      "INFO:tensorflow:training step 95 | Loss: 13.33 Examples/sec: 1369.91\n",
      "INFO:tensorflow:training step 96 | Loss: 13.51 Examples/sec: 1337.40\n",
      "INFO:tensorflow:training step 97 | Loss: 13.39 Examples/sec: 1291.85\n",
      "INFO:tensorflow:training step 98 | Loss: 13.27 Examples/sec: 1314.50\n",
      "INFO:tensorflow:training step 99 | Loss: 13.37 Examples/sec: 1330.57\n",
      "INFO:tensorflow:training step 100 | Loss: 13.24 Examples/sec: 1344.98 | Hit@1: 0.29 PERR: 0.22 GAP: 0.09\n",
      "INFO:tensorflow:training step 101 | Loss: 13.24 Examples/sec: 1488.41\n",
      "INFO:tensorflow:training step 102 | Loss: 13.16 Examples/sec: 1326.57\n",
      "INFO:tensorflow:training step 103 | Loss: 13.23 Examples/sec: 1274.40\n",
      "INFO:tensorflow:training step 104 | Loss: 13.03 Examples/sec: 1313.95\n",
      "INFO:tensorflow:training step 105 | Loss: 13.06 Examples/sec: 1325.30\n",
      "INFO:tensorflow:training step 106 | Loss: 13.09 Examples/sec: 1351.95\n",
      "INFO:tensorflow:training step 107 | Loss: 13.31 Examples/sec: 1592.88\n",
      "INFO:tensorflow:training step 108 | Loss: 13.44 Examples/sec: 1087.93\n",
      "INFO:tensorflow:/job:master/task:0: Done training -- epoch limit reached.\n",
      "INFO:tensorflow:/job:master/task:0: Exited training loop.\n"
     ]
    }
   ],
   "source": [
    "!python youtube-8m/train.py --frame_features --model=LstmModel --feature_names=audio_embedding --feature_sizes=128 --train_data_pattern=audioset_v1_embeddings/bal_train/*.tfrecord --train_dir model_new/dir --start_new_model --base_learning_rate=0.001 --num_epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VGGish Audioset & Auio embedding Tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
